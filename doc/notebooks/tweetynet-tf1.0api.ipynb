{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tweetynet inference graph\n",
    "This notebook uses the original implementation of TweetyNet in the low-level Tensorflow 1.0 API, to walk through the dimensions -- to explain the architecture, and to make sure the Torch implementation is reproducing it + results correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_syllables=10\n",
    "batch_size=11\n",
    "time_bins=88\n",
    "freq_bins=257\n",
    "channels=1\n",
    "conv1_filters=32\n",
    "conv2_filters=64\n",
    "pool1_size=(1, 8)\n",
    "pool1_strides=(1, 8)\n",
    "pool2_size=(1, 8)\n",
    "pool2_strides=(1, 8)\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "def out_width(in_width, filter_width, stride):\n",
    "    return ceil(float(in_width - filter_width + 1) / float(stride))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input shape\n",
    "originally, spectrograms were transposed **before** being fed to the network, so the shape was \n",
    "(batch, time bins, freq bins, 'channel').\n",
    "If you want to think of this an image, the order would have been:\n",
    "(batch, width, height, channel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.random.normal((batch_size, time_bins, freq_bins, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.compat.v1.layers.conv2d(\n",
    "    inputs=tf.reshape(X, [batch_size, -1, freq_bins, 1]),\n",
    "    filters=conv1_filters,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name='conv1')\n",
    "\n",
    "pool1 = tf.compat.v1.layers.max_pooling2d(inputs=conv1,\n",
    "                                pool_size=pool1_size,\n",
    "                                strides=pool1_strides,\n",
    "                                name='pool1')\n",
    "\n",
    "conv2 = tf.compat.v1.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=conv2_filters,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name='conv2')\n",
    "\n",
    "pool2 = tf.compat.v1.layers.max_pooling2d(inputs=conv2,\n",
    "                                pool_size=pool2_size,\n",
    "                                strides=pool2_strides,\n",
    "                                name='pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of conv1: (11, 88, 257, 32)\n",
      "shape of pool1: (11, 88, 32, 32)\n",
      "shape of conv2: (11, 88, 32, 64)\n",
      "shape of pool2: (11, 88, 4, 64)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in zip(\n",
    "    ['conv1', 'pool1', 'conv2', 'pool2'],\n",
    "    [conv1, pool1, conv2, pool2]\n",
    "):\n",
    "    print(f'shape of {name}:', layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshaping input for recurrent network\n",
    "\n",
    "### determining number of hidden units\n",
    "After passing through the convnet, the input spectrogram has been mapped to 64 channels with 8 frequency bins. We stack all of these channels on top of each other to produce a new \"image\" where the rows are \"channel frequencies\". \n",
    "\n",
    "Then we feed this \"image\" to a recurrent neural network with a hidden unit for each \"channel frequency\".\n",
    "\n",
    "Therefore we need the number of hidden units to equal the number of frequency bins left after the downsampling done by the max pooling layers * the number of channels after the convolutional layers.\n",
    "\n",
    "With eager mode + attributes added in later versions of tensorflow, there's no longer a need to determine the output shapes programatically as we do here.\n",
    "\n",
    "It is helpful for people trying to understand the network structure to see the explicit variable names though (`freq_bins_after_pool2` and `conv2_filters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of hidden units in bidirectional LSTM:\n",
    "# uniquely determined by number of filters and frequency bins\n",
    "# in output shape of pool2\n",
    "freq_bins_after_pool1 = out_width(freq_bins,\n",
    "                                  pool1_size[1],\n",
    "                                  pool1_strides[1])\n",
    "freq_bins_after_pool2 = out_width(freq_bins_after_pool1,\n",
    "                                  pool2_size[1],\n",
    "                                  pool2_strides[1])\n",
    "num_hidden = freq_bins_after_pool2 * conv2_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden units: 512\n"
     ]
    }
   ],
   "source": [
    "print('number of hidden units:', num_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic bi-directional LSTM\n",
    "lstm_f1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0,\n",
    "                                       state_is_tuple=True, reuse=None)\n",
    "lstm_b1 = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(num_hidden, forget_bias=1.0,\n",
    "                                       state_is_tuple=True, reuse=None)\n",
    "outputs, _states = tf.compat.v1.nn.bidirectional_dynamic_rnn(lstm_f1,\n",
    "                                                             lstm_b1,\n",
    "                                                             inputs=tf.reshape(pool2, \n",
    "                                                                               [batch_size,\n",
    "                                                                                -1,\n",
    "                                                                                num_hidden]),\n",
    "                                                             time_major=False,\n",
    "                                                             dtype=tf.float32,\n",
    "                                                             # sequence_length=[time_bins],\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the input to the bidirectional LSTM is `pool2`, reshaped so that the dimension order is:  \n",
    "(batch, time_bins, num_hidden)\n",
    "\n",
    "This is where the stacking happens, a reshape hidden inside a function call.\n",
    "\n",
    "We need time bins to be the second axis.\n",
    "\n",
    "It's fine to just use a minus one here because the output of `pool2` already has time bins on the second dimension.\n",
    "We can confirm this by testing whether the results are equal to a reshape where we explicitly specify the second dimension should be of size `time_bins`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_all(\n",
    "    tf.math.equal(\n",
    "        tf.reshape(pool2, [batch_size,-1, num_hidden]),\n",
    "        tf.reshape(pool2, [batch_size, time_bins, num_hidden])\n",
    "    )\n",
    ").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of outputs of bidirectional rnn:\n",
      "forward:  (11, 88, 512)\n",
      "backward:  (11, 88, 512)\n"
     ]
    }
   ],
   "source": [
    "print('shape of outputs of bidirectional rnn:')\n",
    "print('forward: ', outputs[0].shape)\n",
    "print('backward: ', outputs[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## projecting outputs\n",
    "This is the part that is most low-level and hardest to wrap my mind around.\n",
    "\n",
    "But essentially we create a set of weights for the forward direction, a set of weights for the backward pass, and a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection on the number of syllables creates logits time_steps\n",
    "W_f = tf.Variable(tf.random.normal([num_hidden, n_syllables]))\n",
    "W_b = tf.Variable(tf.random.normal([num_hidden, n_syllables]))\n",
    "bias = tf.Variable(tf.random.normal([n_syllables]))\n",
    "\n",
    "expr1 = tf.unstack(outputs[0],\n",
    "                   axis=0,\n",
    "                   num=batch_size)\n",
    "expr2 = tf.unstack(outputs[1],\n",
    "                   axis=0,\n",
    "                   num=batch_size)\n",
    "logits = tf.concat([tf.matmul(ex1, W_f) + bias + tf.matmul(ex2, W_b)\n",
    "                    for ex1, ex2 in zip(expr1, expr2)], 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
