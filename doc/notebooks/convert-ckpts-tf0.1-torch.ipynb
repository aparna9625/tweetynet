{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "# import tweetynet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cutting and pasting model for now to not deal with fixing my environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Conv2dTF(nn.Conv2d):\n",
    "    \"\"\"Conv2d with padding behavior from Tensorflow\n",
    "\n",
    "    adapted from\n",
    "    https://github.com/mlperf/inference/blob/16a5661eea8f0545e04c86029362e22113c2ec09/others/edge/object_detection/ssd_mobilenet/pytorch/utils.py#L40\n",
    "    as referenced in this issue:\n",
    "    https://github.com/pytorch/pytorch/issues/3867#issuecomment-507025011\n",
    "\n",
    "    used to maintain behavior of original implementation of TweetyNet that used Tensorflow 1.0 low-level API\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Conv2dTF, self).__init__(*args, **kwargs)\n",
    "        self.padding = kwargs.get(\"padding\", \"SAME\")\n",
    "\n",
    "    def _compute_padding(self, input, dim):\n",
    "        input_size = input.size(dim + 2)\n",
    "        filter_size = self.weight.size(dim + 2)\n",
    "        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n",
    "        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n",
    "        total_padding = max(\n",
    "            0, (out_size - 1) * self.stride[dim] + effective_filter_size - input_size\n",
    "        )\n",
    "        additional_padding = int(total_padding % 2 != 0)\n",
    "\n",
    "        return additional_padding, total_padding\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.padding == \"VALID\":\n",
    "            return F.conv2d(\n",
    "                input,\n",
    "                self.weight,\n",
    "                self.bias,\n",
    "                self.stride,\n",
    "                padding=0,\n",
    "                dilation=self.dilation,\n",
    "                groups=self.groups,\n",
    "            )\n",
    "        rows_odd, padding_rows = self._compute_padding(input, dim=0)\n",
    "        cols_odd, padding_cols = self._compute_padding(input, dim=1)\n",
    "        if rows_odd or cols_odd:\n",
    "            input = F.pad(input, [0, cols_odd, 0, rows_odd])\n",
    "\n",
    "        return F.conv2d(\n",
    "            input,\n",
    "            self.weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            padding=(padding_rows // 2, padding_cols // 2),\n",
    "            dilation=self.dilation,\n",
    "            groups=self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class TweetyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 input_shape=(1, 513, 88),\n",
    "                 conv1_filters=32,\n",
    "                 conv1_kernel_size=(5, 5),\n",
    "                 conv2_filters=64,\n",
    "                 conv2_kernel_size=(5, 5),\n",
    "                 pool1_size=(8, 1),\n",
    "                 pool1_stride=(8, 1),\n",
    "                 pool2_size=(8, 1),\n",
    "                 pool2_stride=(8, 1),\n",
    "                 ):\n",
    "        \"\"\"initialize TweetyNet model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            number of classes to predict, e.g., number of syllable classes in an individual bird's song\n",
    "        input_shape : tuple\n",
    "            with 3 elements corresponding to dimensions of spectrogram windows: (channels, frequency bins, time bins).\n",
    "            i.e. we assume input is a spectrogram and treat it like an image, typically with one channel,\n",
    "            the rows are frequency bins, and the columns are time bins. Default is (1, 513, 88).\n",
    "        conv1_filters : int\n",
    "            Number of filters in first convolutional layer. Default is 32.\n",
    "        conv1_kernel_size : tuple\n",
    "            Size of kernels, i.e. filters, in first convolutional layer. Default is (5, 5).\n",
    "        conv2_filters : int\n",
    "            Number of filters in second convolutional layer. Default is 64.\n",
    "        conv2_kernel_size : tuple\n",
    "            Size of kernels, i.e. filters, in second convolutional layer. Default is (5, 5).\n",
    "        pool1_size : two element tuple of ints\n",
    "            Size of sliding window for first max pooling layer. Default is (1, 8)\n",
    "        pool1_stride : two element tuple of ints\n",
    "            Step size for sliding window of first max pooling layer. Default is (1, 8)\n",
    "        pool2_size : two element tuple of ints\n",
    "            Size of sliding window for second max pooling layer. Default is (1, 8),\n",
    "        pool2_stride : two element tuple of ints\n",
    "            Step size for sliding window of second max pooling layer. Default is (1, 8)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            Conv2dTF(in_channels=self.input_shape[0],\n",
    "                     out_channels=conv1_filters,\n",
    "                     kernel_size=conv1_kernel_size,\n",
    "                     padding='same'\n",
    "                     ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool1_size,\n",
    "                         stride=pool1_stride),\n",
    "            Conv2dTF(in_channels=conv1_filters,\n",
    "                      out_channels=conv2_filters,\n",
    "                      kernel_size=conv2_kernel_size,\n",
    "                     padding = 'same'\n",
    "                     ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=pool2_size,\n",
    "                         stride=pool2_stride),\n",
    "        )\n",
    "\n",
    "        # determine number of features in output after stacking channels\n",
    "        # we use the same number of features for hidden states\n",
    "        # note self.num_hidden is also used to reshape output of cnn in self.forward method\n",
    "        batch_shape = tuple((1,) + input_shape)\n",
    "        tmp_tensor = torch.rand(batch_shape)\n",
    "        tmp_out = self.cnn(tmp_tensor)\n",
    "        channels_out, freqbins_out = tmp_out.shape[1], tmp_out.shape[2]\n",
    "        self.num_rnn_features = channels_out * freqbins_out\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=self.num_rnn_features,\n",
    "                           hidden_size=self.num_rnn_features,\n",
    "                           num_layers=1,\n",
    "                           dropout=0,\n",
    "                           bidirectional=True)\n",
    "\n",
    "        # for self.fc, in_features = num_rnn_features * 2\n",
    "        # because LSTM is bidirectional\n",
    "        # so we get features forward + features backward as output\n",
    "        self.fc = nn.Linear(self.num_rnn_features * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        # stack channels so that dimension order is (batch, num_rnn_features, num time bins)\n",
    "        features = features.view(features.shape[0], self.num_rnn_features, -1)\n",
    "        # now switch dimensions for feeding to rnn,\n",
    "        # so dimension order is (num time bins, batch size, num_rnn_features)\n",
    "        features = features.permute(2, 0, 1)\n",
    "        rnn_output, (hidden, cell_state) = self.rnn(features)\n",
    "        # permute back to (batch, time bins, features)\n",
    "        # so we can project features down onto number of classes\n",
    "        rnn_output = rnn_output.permute(1, 0, 2)\n",
    "        logits = self.fc(rnn_output)\n",
    "        # permute yet again\n",
    "        # so that dimension order is (batch, classes, time steps)\n",
    "        # because this is order that loss function expects\n",
    "        return logits.permute(0, 2, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate model so we can look at size of parameter arrays.\n",
    "\n",
    "Pick 8 classes because I can see from loaded checkpoint (below) that was the number of classes at output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_net = TweetyNet(num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a checkpoint.\n",
    "\n",
    "Following this blog post from Hugging Face.\n",
    "https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n",
    "\n",
    "Seems to work for `tensorflow` 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(\n",
    "    '~/Documents/data/birdsong/BFSongRepository/vak/bl26lb16/'\n",
    "    'results_191111_084054/TweetyNet/checkpoint_TweetyNet'\n",
    ").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inference/Projection/Variable', [128, 8]),\n",
      " ('inference/Projection/Variable_1', [128, 8]),\n",
      " ('inference/Projection/Variable_2', [8]),\n",
      " ('inference/bidirectional_rnn/bw/basic_lstm_cell/bias', [512]),\n",
      " ('inference/bidirectional_rnn/bw/basic_lstm_cell/kernel', [256, 512]),\n",
      " ('inference/bidirectional_rnn/fw/basic_lstm_cell/bias', [512]),\n",
      " ('inference/bidirectional_rnn/fw/basic_lstm_cell/kernel', [256, 512]),\n",
      " ('inference/conv1/bias', [32]),\n",
      " ('inference/conv1/kernel', [5, 5, 1, 32]),\n",
      " ('inference/conv2/bias', [64]),\n",
      " ('inference/conv2/kernel', [5, 5, 32, 64]),\n",
      " ('optimize/beta1_power', []),\n",
      " ('optimize/beta2_power', []),\n",
      " ('optimize/global_step', []),\n",
      " ('optimize/inference/Projection/Variable/Adam', [128, 8]),\n",
      " ('optimize/inference/Projection/Variable/Adam_1', [128, 8]),\n",
      " ('optimize/inference/Projection/Variable_1/Adam', [128, 8]),\n",
      " ('optimize/inference/Projection/Variable_1/Adam_1', [128, 8]),\n",
      " ('optimize/inference/Projection/Variable_2/Adam', [8]),\n",
      " ('optimize/inference/Projection/Variable_2/Adam_1', [8]),\n",
      " ('optimize/inference/bidirectional_rnn/bw/basic_lstm_cell/bias/Adam', [512]),\n",
      " ('optimize/inference/bidirectional_rnn/bw/basic_lstm_cell/bias/Adam_1', [512]),\n",
      " ('optimize/inference/bidirectional_rnn/bw/basic_lstm_cell/kernel/Adam',\n",
      "  [256, 512]),\n",
      " ('optimize/inference/bidirectional_rnn/bw/basic_lstm_cell/kernel/Adam_1',\n",
      "  [256, 512]),\n",
      " ('optimize/inference/bidirectional_rnn/fw/basic_lstm_cell/bias/Adam', [512]),\n",
      " ('optimize/inference/bidirectional_rnn/fw/basic_lstm_cell/bias/Adam_1', [512]),\n",
      " ('optimize/inference/bidirectional_rnn/fw/basic_lstm_cell/kernel/Adam',\n",
      "  [256, 512]),\n",
      " ('optimize/inference/bidirectional_rnn/fw/basic_lstm_cell/kernel/Adam_1',\n",
      "  [256, 512]),\n",
      " ('optimize/inference/conv1/bias/Adam', [32]),\n",
      " ('optimize/inference/conv1/bias/Adam_1', [32]),\n",
      " ('optimize/inference/conv1/kernel/Adam', [5, 5, 1, 32]),\n",
      " ('optimize/inference/conv1/kernel/Adam_1', [5, 5, 1, 32]),\n",
      " ('optimize/inference/conv2/bias/Adam', [64]),\n",
      " ('optimize/inference/conv2/bias/Adam_1', [64]),\n",
      " ('optimize/inference/conv2/kernel/Adam', [5, 5, 32, 64]),\n",
      " ('optimize/inference/conv2/kernel/Adam_1', [5, 5, 32, 64])]\n"
     ]
    }
   ],
   "source": [
    "tf_vars = tf.train.list_variables(str(ckpt_path))\n",
    "pprint(tf_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only keep the inference vars,\n",
    "we're not worried about the state of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_vars = [\n",
    "    tf_var for tf_var in tf_vars if tf_var[0].startswith('inference')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inference/Projection/Variable', [128, 8]),\n",
       " ('inference/Projection/Variable_1', [128, 8]),\n",
       " ('inference/Projection/Variable_2', [8]),\n",
       " ('inference/bidirectional_rnn/bw/basic_lstm_cell/bias', [512]),\n",
       " ('inference/bidirectional_rnn/bw/basic_lstm_cell/kernel', [256, 512]),\n",
       " ('inference/bidirectional_rnn/fw/basic_lstm_cell/bias', [512]),\n",
       " ('inference/bidirectional_rnn/fw/basic_lstm_cell/kernel', [256, 512]),\n",
       " ('inference/conv1/bias', [32]),\n",
       " ('inference/conv1/kernel', [5, 5, 1, 32]),\n",
       " ('inference/conv2/bias', [64]),\n",
       " ('inference/conv2/kernel', [5, 5, 32, 64])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually not so daunting now that we see there's just a few parameters.\n",
    "\n",
    "CNN will be easiest to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params = list(a_net.cnn.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([32, 1, 5, 5])\n",
      "0.bias torch.Size([32])\n",
      "3.weight torch.Size([64, 32, 5, 5])\n",
      "3.bias torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for p in cnn_params:\n",
    "    print(p[0], p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_var_cnn_param_map = {\n",
    "    'inference/conv1/kernel': a_net.cnn[0].weight,\n",
    "    'inference/conv1/bias': a_net.cnn[0].bias,\n",
    "    'inference/conv2/kernel': a_net.cnn[3].weight,\n",
    "    'inference/conv2/bias': a_net.cnn[3].bias,    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to change dimension order\n",
    "\n",
    "**Also need to rotate / transpose arrays because input to TF had \n",
    "dimension order (time bins, frequencies) while TweetyNet has \n",
    "(frequencies, time bins)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tf_var_name, cnn_param in tf_var_cnn_param_map.items():\n",
    "    array = tf.train.load_variable(str(ckpt_path), tf_var_name)\n",
    "    # cnn_param.data = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_params = list(a_net.rnn.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([2048, 512])\n",
      "weight_hh_l0 torch.Size([2048, 512])\n",
      "bias_ih_l0 torch.Size([2048])\n",
      "bias_hh_l0 torch.Size([2048])\n",
      "weight_ih_l0_reverse torch.Size([2048, 512])\n",
      "weight_hh_l0_reverse torch.Size([2048, 512])\n",
      "bias_ih_l0_reverse torch.Size([2048])\n",
      "bias_hh_l0_reverse torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "for p in rnn_params:\n",
    "    print(p[0], p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
